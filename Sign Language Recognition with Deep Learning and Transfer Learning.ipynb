{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOIOC2x1F2CTboBB4EOCUwy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AbelAdissu/8-Bit-Computer-Architecture-and-Organization-Project/blob/main/Sign%20Language%20Recognition%20with%20Deep%20Learning%20and%20Transfer%20Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qmkPpZhSg1Wv",
        "outputId": "ae293ebd-ae14-4f27-96a4-50de818391fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opendatasets in /usr/local/lib/python3.10/dist-packages (0.1.22)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from opendatasets) (4.66.1)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (from opendatasets) (1.5.16)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from opendatasets) (8.1.7)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (1.16.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2023.11.17)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2.31.0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (8.0.3)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2.0.7)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (6.1.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle->opendatasets) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle->opendatasets) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle->opendatasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle->opendatasets) (3.6)\n",
            "Skipping, found downloaded files in \"./sign-language-mnist\" (use force=True to force download)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "151/151 [==============================] - 10s 53ms/step - loss: 3.1033 - accuracy: 0.1940 - val_loss: 4.0439 - val_accuracy: 0.0444 - lr: 0.0010\n",
            "Epoch 2/50\n",
            "151/151 [==============================] - 7s 44ms/step - loss: 1.9541 - accuracy: 0.4040 - val_loss: 5.8466 - val_accuracy: 0.0829 - lr: 0.0010\n",
            "Epoch 3/50\n",
            "151/151 [==============================] - 8s 52ms/step - loss: 1.4095 - accuracy: 0.5438 - val_loss: 8.9542 - val_accuracy: 0.0652 - lr: 0.0010\n",
            "Epoch 4/50\n",
            "151/151 [==============================] - 7s 49ms/step - loss: 1.1294 - accuracy: 0.6254 - val_loss: 0.7658 - val_accuracy: 0.7630 - lr: 0.0010\n",
            "Epoch 5/50\n",
            "151/151 [==============================] - 7s 44ms/step - loss: 0.9440 - accuracy: 0.6848 - val_loss: 0.3848 - val_accuracy: 0.8873 - lr: 0.0010\n",
            "Epoch 6/50\n",
            "151/151 [==============================] - 7s 45ms/step - loss: 0.8258 - accuracy: 0.7253 - val_loss: 0.2552 - val_accuracy: 0.9202 - lr: 0.0010\n",
            "Epoch 7/50\n",
            "151/151 [==============================] - 8s 53ms/step - loss: 0.7381 - accuracy: 0.7545 - val_loss: 0.3553 - val_accuracy: 0.8871 - lr: 0.0010\n",
            "Epoch 8/50\n",
            "151/151 [==============================] - 7s 45ms/step - loss: 0.6770 - accuracy: 0.7763 - val_loss: 0.2428 - val_accuracy: 0.9255 - lr: 0.0010\n",
            "Epoch 9/50\n",
            " 45/151 [=======>......................] - ETA: 6s - loss: 0.6589 - accuracy: 0.7811"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "!pip install opendatasets\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import keras\n",
        "from keras.applications import VGG16, ResNet50, InceptionV3\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout, BatchNormalization, GlobalAveragePooling2D\n",
        "from keras.optimizers import Adam\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "import opendatasets as od\n",
        "import tensorflow as tf\n",
        "# Function to select the default device (GPU if available, else CPU)\n",
        "def get_default_device():\n",
        "    if tf.config.list_physical_devices('GPU'):\n",
        "        return tf.device(\"GPU\")\n",
        "    else:\n",
        "        return tf.device(\"CPU\")\n",
        "\n",
        "# Function to move data to the selected device\n",
        "def to_device(data, device):\n",
        "    with tf.device(device.name):\n",
        "        return data\n",
        "# Call the function to get the default device\n",
        "my_device = get_default_device()\n",
        "\n",
        "# Download the dataset using opendatasets\n",
        "od.download('https://www.kaggle.com/datamunge/sign-language-mnist')\n",
        "\n",
        "# Load the dataset\n",
        "train = pd.read_csv('./sign-language-mnist/sign_mnist_train.csv')\n",
        "test = pd.read_csv('./sign-language-mnist/sign_mnist_test.csv')\n",
        "\n",
        "# Explore the dataset\n",
        "labels = train['label'].values\n",
        "unique_labels = np.unique(labels)\n",
        "\n",
        "# Data exploration\n",
        "plt.figure(figsize=(18, 8))\n",
        "sns.countplot(x=labels)\n",
        "plt.title(\"Distribution of Sign Language Labels\")\n",
        "\n",
        "# Drop the label column from the training set\n",
        "train.drop('label', axis=1, inplace=True)\n",
        "\n",
        "# Reshape the images into a format suitable for the model\n",
        "images = train.values\n",
        "images = np.array([np.reshape(i, (28, 28)) for i in images])\n",
        "images = np.array([i.flatten() for i in images])\n",
        "\n",
        "# Encode labels using LabelBinarizer to convert them into a suitable format\n",
        "label_binarizer = LabelBinarizer()\n",
        "labels = label_binarizer.fit_transform(labels)\n",
        "\n",
        "# Split the dataset into train (70%) and test (30%)\n",
        "x_train, x_test, y_train, y_test = train_test_split(images, labels, test_size=0.3, random_state=101)\n",
        "\n",
        "# Normalize the pixel values between 0 and 1 for better model training\n",
        "x_train = x_train / 255\n",
        "x_test = x_test / 255\n",
        "\n",
        "# Reshape images to match the input shape expected by the models\n",
        "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
        "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
        "\n",
        "# Data Augmentation: Augment the training data to create new training samples\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# Learning Rate Schedulers: Implement learning rate schedulers\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=0.00001)\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "# Define constants for the models\n",
        "batch_size = 128  # Define batch size here\n",
        "num_classes = 24\n",
        "epochs = 50\n",
        "\n",
        "# Apply Data Augmentation to the training data\n",
        "augmented_data = datagen.flow(x_train, y_train, batch_size=batch_size)\n",
        "\n",
        "# Re-build and compile the custom CNN model with Batch Normalization\n",
        "model = Sequential()\n",
        "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(lr=0.001),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model with Data Augmentation and Learning Rate Schedulers\n",
        "history = model.fit(augmented_data, validation_data=(x_test, y_test), epochs=epochs, batch_size=batch_size,\n",
        "                    callbacks=[reduce_lr, early_stopping])\n",
        "# Batch Normalization: Add BatchNormalization layers\n",
        "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "# Different CNN Architectures: Experiment with VGG16, ResNet50, and InceptionV3\n",
        "vgg16_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "vgg16_model = to_device(vgg16_model, my_device)\n",
        "#vgg16_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "# Build and train the ResNet50 model\n",
        "resnet50_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "resnet50_model = to_device(resnet50_model, my_device)\n",
        "#resnet50_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "inceptionv3_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "inceptionv3_model = to_device(inceptionv3_model, my_device)\n",
        "# Dropout Rate Tuning: Fine-tune the dropout rate\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "# Regularization: Add L2 regularization\n",
        "from keras.regularizers import l2\n",
        "model.add(Conv2D(128, kernel_size=(3, 3), activation='relu', kernel_regularizer=l2(0.01)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "# Data Balancing: If needed, apply data balancing techniques\n",
        "# Oversampling, Undersampling, or Class Weighting\n",
        "\n",
        "# Continue with model compilation, training, and evaluation...\n",
        "# Re-compile the model after adding Batch Normalization, Dropout Rate Tuning, Regularization, etc.\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(lr=0.001),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(augmented_data, validation_data=(x_test, y_test), epochs=epochs, batch_size=batch_size,\n",
        "                    callbacks=[reduce_lr, early_stopping])\n",
        "\n",
        "# Evaluate the model on the test data\n",
        "test_labels = test['label']\n",
        "test.drop('label', axis=1, inplace=True)\n",
        "test_images = test.values\n",
        "test_images = np.array([np.reshape(i, (28, 28)) for i in test_images])\n",
        "test_images = np.array([i.flatten() for i in test_images])\n",
        "test_labels = label_binrizer.fit_transform(test_labels)\n",
        "test_images = test_images.reshape(test_images.shape[0], 28, 28, 1)\n",
        "\n",
        "# Predict using the model\n",
        "y_pred = model.predict(test_images)\n",
        "\n",
        "# Calculate accuracy for the model\n",
        "accuracy = accuracy_score(test_labels, y_pred.round())\n",
        "print(\"Model Test Accuracy:\", accuracy)\n",
        "\n",
        "# Calculate other evaluation metrics (confusion matrix, recall, precision)\n",
        "y_true = np.argmax(test_labels, axis=1)\n",
        "y_pred = np.argmax(y_pred, axis=1)\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "cr = classification_report(y_true, y_pred, target_names=[str(i) for i in range(num_classes)])\n",
        "\n",
        "# Print confusion matrix, recall, and precision\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n",
        "print(\"\\nClassification Report:\")\n",
        "print(cr)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IqMUGB4QhZpG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}